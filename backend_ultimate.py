#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Ultimate System v21 Final - í¬ë¡¤ëŸ¬ í˜•ì‹ í˜¸í™˜ ë²„ì „
ëª¨ë“  í¬ë¡¤ëŸ¬ ë°˜í™˜ í˜•ì‹ê³¼ í˜¸í™˜
"""

import schedule
import time
import logging
from datetime import datetime
from pathlib import Path
import sqlite3

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('ultimate_system_v21.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class UltimateSystemV21Final:
    """ì™„ì „ í†µí•© ìë™í™” ì‹œìŠ¤í…œ - ìµœì¢… ë²„ì „"""

    def __init__(self):
        self.base_path = Path(__file__).parent
        self.raw_db = self.base_path / 'chart_rankings.db'
        self.final_db = self.base_path / 'rank_history.db'

        # í¬ë¡¤ëŸ¬ ì„¤ì • (í´ë˜ìŠ¤ëª… í¬í•¨)
        self.crawler_config = {
            'melon': {'file': 'melon_crawler', 'class': 'MelonCrawler'},     # â† ë¬¸ì œ
            'genie': {'file': 'genie_crawler', 'class': 'GenieCrawler'},     # â† ë¬¸ì œ
            'bugs': {'file': 'bugs_crawler', 'class': 'BugsCrawler'},       # â† ë¬¸ì œ
            'flo': {'file': 'flo_crawler', 'class': 'FLOCrawler'},               # â† ì •ìƒ
            'spotify': {'file': 'spotify_crawler', 'class': 'SpotifyCrawler'},   # â† ì •ìƒ
            'apple_music': {'file': 'apple_music_crawler', 'class': 'AppleMusicCrawler'},
            'lastfm': {'file': 'lastfm_crawler', 'class': 'LastFMCrawler'},
            'youtube': {'file': 'youtube_crawler', 'class': 'YouTubeCrawler'}
        }

        logger.info("Ultimate System v21 Final ì´ˆê¸°í™” ì™„ë£Œ")

    def ensure_tables(self):
        """í•„ìš”í•œ í…Œì´ë¸” í™•ì¸ ë° ìƒì„±"""
        conn = sqlite3.connect(str(self.final_db))
        cursor = conn.cursor()

        # chart_snapshots í…Œì´ë¸”
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS chart_snapshots (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                chart_name TEXT NOT NULL,
                rank_position INTEGER NOT NULL,
                artist TEXT NOT NULL,
                track TEXT NOT NULL,
                album TEXT,
                image_url TEXT,
                views_or_streams TEXT,
                snapshot_time TIMESTAMP NOT NULL,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        """)

        cursor.execute("""
            CREATE INDEX IF NOT EXISTS idx_chart_time
            ON chart_snapshots(chart_name, snapshot_time)
        """)

        # chart_metadata í…Œì´ë¸”
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS chart_metadata (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                chart_name TEXT UNIQUE NOT NULL,
                last_update TIMESTAMP,
                total_tracks INTEGER,
                is_active BOOLEAN DEFAULT 1
            )
        """)

        # unified_master_with_images (UNIQUE ì œì•½ ì—†ì´)
        cursor.execute("""
            SELECT sql FROM sqlite_master
            WHERE type='table' AND name='unified_master_with_images'
        """)

        schema = cursor.fetchone()
        if not schema or "UNIQUE" in str(schema[0] if schema else "").upper():
            cursor.execute("DROP TABLE IF EXISTS unified_master_with_images")
            cursor.execute("""
                CREATE TABLE unified_master_with_images (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    chart_name TEXT,
                    rank_position INTEGER,
                    unified_artist TEXT,
                    unified_track TEXT,
                    original_artist TEXT,
                    original_track TEXT,
                    image_url TEXT,
                    views_or_streams TEXT,
                    local_image TEXT,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            """)

            cursor.execute("""
                CREATE INDEX IF NOT EXISTS idx_chart_rank
                ON unified_master_with_images(chart_name, rank_position)
            """)

        conn.commit()
        conn.close()

    def run_crawler(self, chart_name):
        """ê°œë³„ í¬ë¡¤ëŸ¬ ì‹¤í–‰ - ëª¨ë“  ë°˜í™˜ í˜•ì‹ê³¼ í˜¸í™˜"""
        try:
            config = self.crawler_config.get(chart_name)
            if not config:
                logger.warning(f"âŒ {chart_name}: ì„¤ì • ì—†ìŒ")
                return False

            logger.info(f"ğŸ•·ï¸ {chart_name} í¬ë¡¤ë§ ì‹œì‘...")

            # í¬ë¡¤ëŸ¬ import
            from importlib import import_module
            module = import_module(f'crawlers.{config["file"]}')
            crawler_class = getattr(module, config['class'])
            crawler = crawler_class()

            # í¬ë¡¤ë§ ì‹¤í–‰
            result = crawler.crawl()

            # ë°˜í™˜ í˜•ì‹ ì²˜ë¦¬ (dict ë˜ëŠ” list ëª¨ë‘ ì§€ì›)
            if isinstance(result, dict):
                # {'data': [...]} í˜•ì‹
                data = result.get('data', [])
            elif isinstance(result, list):
                # ë¦¬ìŠ¤íŠ¸ ì§ì ‘ ë°˜í™˜
                data = result
            else:
                logger.warning(f"âŒ {chart_name}: ì•Œ ìˆ˜ ì—†ëŠ” ë°˜í™˜ í˜•ì‹")
                return False

            if data:
                self.save_to_raw_db(chart_name, data)
                logger.info(f"âœ… {chart_name}: {len(data)}ê°œ í¬ë¡¤ë§ ì™„ë£Œ")

                # ê³ í™”ì§ˆ ì´ë¯¸ì§€ ì²˜ë¦¬ ì‹œë„
                self.try_high_quality_images(chart_name, data[:10])

                return True
            else:
                logger.warning(f"âŒ {chart_name}: ë°ì´í„° ì—†ìŒ")
                return False

        except Exception as e:
            logger.error(f"âŒ {chart_name} í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return False

    def try_high_quality_images(self, chart_name, data):
        """ê³ í™”ì§ˆ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹œë„"""
        try:
            from local_image_manager import get_image_manager
            manager = get_image_manager()

            success = 0
            for item in data:
                if 'image_url' in item and item['image_url']:
                    if manager.download_and_save(
                        item['image_url'],
                        item.get('artist', ''),
                        item.get('track', item.get('title', ''))
                    ):
                        success += 1

            if success > 0:
                logger.info(f"  ğŸ–¼ï¸ {success}ê°œ ê³ í™”ì§ˆ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ")
        except:
            pass  # ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰

    def save_to_raw_db(self, chart_name, data):
        """Raw DBì— ì €ì¥"""
        conn = sqlite3.connect(str(self.raw_db))
        cursor = conn.cursor()

        crawled_at = datetime.now().isoformat()

        for item in data:
            # track ë˜ëŠ” title ì‚¬ìš©
            track_name = item.get('track') or item.get('title', '')

            cursor.execute("""
                INSERT INTO raw_chart_data
                (chart_name, rank, title, artist, track, image_url,
                 views_or_streams, streams, crawled_at)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                chart_name,
                item.get('rank'),
                track_name,
                item.get('artist', ''),
                track_name,
                item.get('image_url', ''),
                item.get('views_or_streams', ''),
                item.get('streams', ''),
                crawled_at
            ))

        conn.commit()
        conn.close()

    def process_to_timeseries(self, chart_name):
        """ì‹œê³„ì—´ ì²˜ë¦¬"""
        logger.info(f"ğŸ“Š {chart_name} ì‹œê³„ì—´ ì²˜ë¦¬...")

        conn_raw = sqlite3.connect(str(self.raw_db))
        conn_final = sqlite3.connect(str(self.final_db))

        cursor_raw = conn_raw.cursor()
        cursor_final = conn_final.cursor()

        # ìµœì‹  í¬ë¡¤ë§ ê°€ì ¸ì˜¤ê¸°
        cursor_raw.execute("""
            SELECT DISTINCT crawled_at
            FROM raw_chart_data
            WHERE chart_name = ?
            ORDER BY crawled_at DESC
            LIMIT 1
        """, (chart_name,))

        latest_crawl = cursor_raw.fetchone()
        if not latest_crawl:
            conn_raw.close()
            conn_final.close()
            return False

        latest_crawl = latest_crawl[0]
        time_prefix = latest_crawl[:19]

        # ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        cursor_raw.execute("""
            SELECT
                rank,
                artist,
                COALESCE(track, title, '') as track_name,
                image_url,
                COALESCE(views_or_streams, streams, '') as views_data
            FROM raw_chart_data
            WHERE chart_name = ?
            AND crawled_at LIKE ?
            ORDER BY rank
        """, (chart_name, time_prefix + '%'))

        tracks = cursor_raw.fetchall()

        # chart_snapshotsì— ì €ì¥
        saved = 0
        for rank, artist, track, image_url, views_data in tracks:
            if not artist or not track:
                continue

            cursor_final.execute("""
                INSERT INTO chart_snapshots
                (chart_name, rank_position, artist, track, album,
                 image_url, views_or_streams, snapshot_time)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                chart_name,
                rank if rank else saved + 1,
                artist.strip(),
                track.strip(),
                '',
                image_url or '',
                views_data or '',
                latest_crawl
            ))
            saved += 1

        conn_final.commit()

        # metadata ì—…ë°ì´íŠ¸
        cursor_final.execute("""
            INSERT OR REPLACE INTO chart_metadata
            (chart_name, last_update, total_tracks, is_active)
            VALUES (?, ?, ?, 1)
        """, (chart_name, latest_crawl, saved))

        conn_final.commit()
        logger.info(f"âœ… {chart_name}: {saved}ê°œ ì €ì¥")

        conn_raw.close()
        conn_final.close()

        return saved > 0

    def update_unified_master(self):
        """unified_master ì—…ë°ì´íŠ¸"""
        logger.info("ğŸ”„ unified_master ì—…ë°ì´íŠ¸...")

        conn = sqlite3.connect(str(self.final_db))
        cursor = conn.cursor()

        # ì´ˆê¸°í™”
        cursor.execute("DELETE FROM unified_master_with_images")

        # ê° ì°¨íŠ¸ ìµœì‹  ë°ì´í„° ì‚½ì…
        for chart_name in self.crawler_config.keys():
            cursor.execute("""
                INSERT INTO unified_master_with_images
                (chart_name, rank_position, unified_artist, unified_track,
                 original_artist, original_track, image_url, views_or_streams, created_at)
                SELECT
                    chart_name,
                    rank_position,
                    artist,
                    track,
                    artist,
                    track,
                    image_url,
                    views_or_streams,
                    snapshot_time
                FROM chart_snapshots
                WHERE chart_name = ?
                AND snapshot_time = (
                    SELECT MAX(snapshot_time)
                    FROM chart_snapshots
                    WHERE chart_name = ?
                )
                ORDER BY rank_position
            """, (chart_name, chart_name))

        conn.commit()

        # ê²°ê³¼ í™•ì¸
        cursor.execute("""
            SELECT chart_name, COUNT(*), COUNT(DISTINCT unified_track)
            FROM unified_master_with_images
            GROUP BY chart_name
        """)

        for chart, total, unique in cursor.fetchall():
            logger.info(f"  {chart}: {total}ê°œ ({unique}ê°œ ê³ ìœ )")

        conn.close()

    def run_complete_pipeline(self, chart_name=None):
        """ì™„ì „ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        logger.info("=" * 60)
        logger.info(f"ğŸš€ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰: {chart_name or 'ì „ì²´'}")
        logger.info("=" * 60)

        # í…Œì´ë¸” í™•ì¸
        self.ensure_tables()

        # ì°¨íŠ¸ ëª©ë¡
        charts = [chart_name] if chart_name else list(self.crawler_config.keys())

        success_count = 0
        for chart in charts:
            try:
                # 1. í¬ë¡¤ë§
                if self.run_crawler(chart):
                    # 2. ì‹œê³„ì—´ ì²˜ë¦¬
                    if self.process_to_timeseries(chart):
                        success_count += 1
            except Exception as e:
                logger.error(f"âŒ {chart} ì‹¤íŒ¨: {e}")

        # 3. unified_master ì—…ë°ì´íŠ¸
        if success_count > 0:
            self.update_unified_master()

            # 4. ìºì‹œ ê°±ì‹ 
            try:
                from smart_cache_warmer_v3 import SmartCacheWarmer
                cache_warmer = SmartCacheWarmer()
                cache_warmer.warm_all_cache()
                logger.info("âœ… ìºì‹œ ê°±ì‹ ")
            except:
                pass

        logger.info(f"âœ… ì™„ë£Œ: {success_count}/{len(charts)} ì„±ê³µ")
        return success_count > 0

def test_mode():
    """í…ŒìŠ¤íŠ¸ ëª¨ë“œ"""
    print("=" * 60)
    print("ğŸ§ª Ultimate System v21 Final í…ŒìŠ¤íŠ¸")
    print("=" * 60)

    system = UltimateSystemV21Final()

    # ë©œë¡  í…ŒìŠ¤íŠ¸
    success = system.run_complete_pipeline("melon")

    if success:
        print("\nâœ… í…ŒìŠ¤íŠ¸ ì„±ê³µ!")
        print("\nê²°ê³¼ í™•ì¸:")
        print("python app.py")
        print("curl http://localhost:5000/api/charts/melon")
    else:
        print("\nâŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")

if __name__ == "__main__":
    import sys

    if len(sys.argv) > 1 and sys.argv[1] == "test":
        test_mode()
    else:
        system = UltimateSystemV21Final()
        # ì´ˆê¸° ì‹¤í–‰ (ì£¼ì„ ìœ ì§€)
        # system.run_complete_pipeline()

        # ìŠ¤ì¼€ì¤„ ì„¤ì • ë° ì‹¤í–‰
        import schedule

        # ìŠ¤ì¼€ì¤„ ì„¤ì • - ì „ì²´ 7ê°œ í¬ë¡¤ëŸ¬
        # Melon (4íšŒ/ì¼)
        schedule.every().day.at("01:10").do(system.run_complete_pipeline, "melon")
        schedule.every().day.at("07:10").do(system.run_complete_pipeline, "melon")
        schedule.every().day.at("13:10").do(system.run_complete_pipeline, "melon")
        schedule.every().day.at("19:10").do(system.run_complete_pipeline, "melon")

        # Genie (4íšŒ/ì¼)
        schedule.every().day.at("01:15").do(system.run_complete_pipeline, "genie")
        schedule.every().day.at("07:15").do(system.run_complete_pipeline, "genie")
        schedule.every().day.at("13:15").do(system.run_complete_pipeline, "genie")
        schedule.every().day.at("19:15").do(system.run_complete_pipeline, "genie")

        # Bugs (4íšŒ/ì¼)
        schedule.every().day.at("01:20").do(system.run_complete_pipeline, "bugs")
        schedule.every().day.at("07:20").do(system.run_complete_pipeline, "bugs")
        schedule.every().day.at("13:20").do(system.run_complete_pipeline, "bugs")
        schedule.every().day.at("19:20").do(system.run_complete_pipeline, "bugs")

        # FLO (2íšŒ/ì¼)
        schedule.every().day.at("01:25").do(system.run_complete_pipeline, "flo")
        schedule.every().day.at("13:25").do(system.run_complete_pipeline, "flo")

        # Apple Music (1íšŒ/ì¼)
        schedule.every().day.at("09:10").do(system.run_complete_pipeline, "apple_music")

        # Spotify (1íšŒ/ì¼)
        schedule.every().day.at("10:10").do(system.run_complete_pipeline, "spotify")

        # Last.fm (1íšŒ/ì¼)
        schedule.every().day.at("12:10").do(system.run_complete_pipeline, "lastfm")

        logger.info("ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘ - 7ê°œ í¬ë¡¤ëŸ¬ ë“±ë¡ ì™„ë£Œ")
        logger.info("ë“±ë¡ëœ í¬ë¡¤ëŸ¬: Melon(4íšŒ), Genie(4íšŒ), Bugs(4íšŒ), FLO(2íšŒ), Apple(1íšŒ), Spotify(1íšŒ), Lastfm(1íšŒ)")
        logger.info("ë‹¤ìŒ í¬ë¡¤ë§: FLO(13:25), Apple(09:10), Spotify(10:10), Lastfm(12:10)")

        # ë©”ì¸ ë£¨í”„
        while True:
            schedule.run_pending()
            time.sleep(60)
